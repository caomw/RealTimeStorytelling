{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 16, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timeseries comparison\n",
    "\n",
    "We can compare pairs of time series (e.g. rates) as long as the sampling times for the two series are (roughly) the same. Below we make two almost identical series `A` and `B`, and look at their correlation. Even though we corrupt B with some noise the correlation is still pretty high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = np.arange(0,20,0.1)\n",
    "A = [np.sin(x)**2 for x in time]\n",
    "sns.tsplot(A,interpolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = [np.sin(x)**2 + np.random.exponential(0.2)  for x in time]\n",
    "sns.tsplot(B,interpolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi, p = scipy.stats.pearsonr(A,B)\n",
    "print phi # correlation is between -1 and 1. -1 means that one series goes up when the other goes down. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we corrupt B with noise, the lower the correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genB = lambda r: [np.sin(x)**2 + np.random.exponential(r)  for x in time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corruption = np.linspace(0.1, 3, 100)\n",
    "d = pd.DataFrame({\"scale\": corruption, \"correlation\": [scipy.stats.pearsonr(A,genB(r))[0] for r in corruption]})\n",
    "sns.regplot(\"scale\",\"correlation\",d,fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also show that, by simply delaying the sine wave by an increasing amount, the correlation moves from 1 to -1, and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genC = lambda phi: [np.sin(x + phi)**2 for x in np.arange(0,20,0.1)]\n",
    "#data = pd.DataFrame({\"0-phase\":genC(0), \"pi-phase\":genC(np.pi), \"time\":time})\n",
    "#data = pd.melt(data,id_vars=[\"time\"])\n",
    "\n",
    "#sns.tsplot(time=\"time\",data=data, value=\"value\", condition=\"variable\", interpolate=False,err_style=None)\n",
    "plt.figure()\n",
    "plt.plot(genC(0))\n",
    "plt.plot(genC(np.pi/4))\n",
    "plt.plot(genC(np.pi/2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phase = np.linspace(0,np.pi,100)\n",
    "d = pd.DataFrame({\"phase\": phase, \"correlation\": [scipy.stats.pearsonr(A,genC(phi))[0] for phi in phase]})\n",
    "sns.regplot(\"phase\",\"correlation\",d,fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distribution comparison\n",
    "\n",
    "If we're interested in comparing how a variable is distributed in two different streams, we can compare their histograms using a number known as the \"Kullback Leibler divergence\", normally referred to as D_KL. Let's make a histogram P and compare it to another histogram Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "categories = range(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can pull a random categorical distribution from the Dirichlet, which is a distribution over vectors that sum to 1. \n",
    "# This is super cool but don't worry too much about it!\n",
    "P = np.random.dirichlet([1 for i in categories])\n",
    "sns.barplot(x=\"categories\", y=\"probability\", data=pd.DataFrame({\"probability\":P, \"categories\":categories}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = np.random.dirichlet([1 for i in categories])\n",
    "sns.barplot(x=\"categories\", y=\"probability\", data=pd.DataFrame({\"probability\":Q, \"categories\":categories}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DKL = lambda p,q : sum(p[i] * np.log(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Kullback Leibler Divergence IS NOT A DISTANCE! It's not symmetric, amongst other things. We can use the symmetrised DKL, which is just the sum. This still isn't a distance, but it's symmetric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print DKL(P,Q), DKL(Q,P), DKL(P,Q)+DKL(Q,P)\n",
    "print scipy.stats.entropy(P,Q) ## note that scipy.stats.entropy will also give you the KL divergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we make a distribution that is markedly different, we should see a higher divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = np.random.dirichlet([np.exp(i+1) for i in categories])\n",
    "sns.barplot(x=\"categories\", y=\"probability\", data=pd.DataFrame({\"probability\":R, \"categories\":categories}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DKL(P,R)+DKL(R,P), DKL(R,Q)+DKL(Q,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
